{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP TEXT MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme $Naïve$ $Bayes$ est une méthode plus ou moins intuitive qui s'appuie sur les probabilités qu'un attribut puisse appartenir à une classe spécifique pour faire une prédiction. C'est une méthode d'apprentissage supervisé plutôt aisée à mettre en place puisque l'on suppose que chaque attribut d'une classe est indépendant des autres.\n",
    "\n",
    "Ce classifieur porte le nom de Bayes car elle repose sur le principe des probabilités conditionnelles développées par Bayes au XVIIIème siècle. En multipliant toutes les probabilités conditionnelles pour chaque attribut d'une classe donnée, on obtient la probablité qu'une nouvelle instance appartienne à cette classe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, Image\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from pylab import *\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer on récupère les fichiers `.txt` qui contiennent les critiques de films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "path = 'D:/WORK/Big_Data/MDI343/'\n",
    "\n",
    "filenames_neg = sorted(glob(op.join(path, 'data_TextMining', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join(path, 'data_TextMining', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rassemble tous les documents 'positifs' et 'négatifs' dans une seule liste `texts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = texts_neg + texts_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit un vecteur de labels $y$ ayant pour valeur $0$ pour les documents `texts_neg` et $1$ pour les autres documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 documents\n",
      "[0 0 0 ..., 1 1 1]\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "y = np.ones(len(texts), dtype=np.int)\n",
    "#\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))\n",
    "print(y) #liste des labels des documents\n",
    "labels = set(y)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** : Implémentation du **Wordcount**. On veut compter le nombre d'occurrence de tous les mots dans tous les documents. Le but est de stocker le résultat dans une matrice qui aura comme dimension (le nombre de documents)x(le nombre de mots sans répétition). Chaque composante de la matrice sera l'occurrence d'un mot dans un document particulier. On va également construire le dictionnaire `vocabulary` qui prend comme clé un mot et en valeur l'indice de la colonne dans `counts` définie par ce mot. On peut définir également un set `word` qui listera de façon unique tous les mots de tous les documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps on définit une fonction `count_word` qui sert à compter les occurrences d'un mot dans un document. Elle retourne une array de paires (mot, occurrence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_word(doc):\n",
    "    vocabulary = {}#dictionnaire contenant un mot du document et son indice colonne dans la matrice count\n",
    "    words = set(' '.join(doc).split())#liste sans répétition de mots du documents\n",
    "    n_samples = len(doc)\n",
    "        \n",
    "    for j,w in enumerate(words):\n",
    "        vocabulary[w] = j\n",
    "        \n",
    "    n_features = len(words)\n",
    "    counts = np.zeros((n_samples,n_features))\n",
    "    \n",
    "    for index_text, text in enumerate(doc):\n",
    "        for w in text.split():\n",
    "            counts[index_text, vocabulary[w]] += 1\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également définir une fonction qui va nettoyer les données: enlever toute la ponctuation et les sauts de ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(doc):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text_clean = []\n",
    "    for d in doc:\n",
    "        d = regex.sub(' ', d)\n",
    "        d = re.sub('\\n', '', d)\n",
    "        text_clean.append(d)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la fonction `clean_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot   two teen couples go to a church party   drink and then drive   they get into an accident   one of the guys dies   but his girlfriend continues to see him in her life   and has nightmares   what s the deal   watch the movie and   sorta   find out       critique   a mind fuck movie for the teen generation that touches on a very cool idea   but presents it in a very bad package   which is what makes this review an even harder one to write   since i generally applaud films which attempt to break the mold   mess with your head and such   lost highway   memento     but there are good and bad ways of making all types of films   and these folks just didn t snag this one correctly   they seem to have taken this pretty neat concept   but executed it terribly   so what are the problems with the movie   well   its main problem is that it s simply too jumbled   it starts off   normal   but then downshifts into this   fantasy   world in which you   as an audience member   have no idea what s going on   there are dreams   there are characters coming back from the dead   there are others who look like the dead   there are strange apparitions   there are disappearances   there are a looooot of chase scenes   there are tons of weird things that happen   and most of it is simply not explained   now i personally don t mind trying to unravel a film every now and then   but when all it does is give me the same clue over and over again   i get kind of fed up after a while   which is this film s biggest problem   it s obviously got this big secret to hide   but it seems to want to hide it completely until its final five minutes   and do they make things entertaining   thrilling or even engaging   in the meantime   not really   the sad part is that the arrow and i both dig on flicks like this   so we actually figured most of it out by the half way point   so all of the strangeness after that did start to make a little bit of sense   but it still didn t the make the film all that more entertaining   i guess the bottom line with movies like this is that you should always make sure that the audience is   into it   even before they are given the secret password to enter your world of understanding   i mean   showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy     okay   we get it       there are people chasing her and we don t know who they are   do we really need to see it over and over again   how about giving us different scenes offering further insight into all of the strangeness going down in the movie   apparently   the studio took this film away from its director and chopped it up themselves   and it shows   there might ve been a pretty decent teen mind fuck movie in here somewhere   but i guess   the suits   decided that turning it into a music video with little edge   would make more sense   the actors are pretty good for the most part   although wes bentley just seemed to be playing the exact same character that he did in american beauty   only in a new neighborhood   but my biggest kudos go out to sagemiller   who holds her own throughout the entire film   and actually has you feeling her character s unraveling   overall   the film doesn t stick because it doesn t entertain   it s confusing   it rarely excites and it feels pretty redundant for most of its runtime   despite a pretty cool ending and explanation to all of the craziness that came before it   oh   and by the way   this is not a horror or teen slasher flick       it s just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids   it also wrapped production two years ago and has been sitting on the shelves ever since   whatever       skip it   where s joblo coming from   a nightmare of elm street 3   7 10     blair witch 2   7 10     the crow   9 10     the crow   salvation   4 10     lost highway   10 10     memento   10 10     the others   9 10     stir of echoes   8 10   ',\n",
       " 'the happy bastard s quick movie review damn that y2k bug   it s got a head start in this movie starring jamie lee curtis and another baldwin brother   william this time   in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on   little do they know the power within       going for the gore and bringing on a few action sequences here and there   virus still feels very empty   like a movie going for all flash and no substance   we don t know why the crew was really out in the middle of nowhere   we don t know the origin of what took over the ship   just that a big pink flashy thing hit the mir     and   of course   we don t know why donald sutherland is stumbling around drunkenly throughout   here   it s just   hey   let s chase these people around with some robots     the acting is below average   even from the likes of curtis   you re more likely to get a kick out of her work in halloween h20   sutherland is wasted and baldwin   well   he s acting like a baldwin   of course   the real star here are stan winston s robot design   some schnazzy cgi   and the occasional good gore shot   like picking into someone s brain   so   if robots and body parts really turn you on   here s your movie   otherwise   it s pretty much a sunken ship of a movie   ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(texts[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la fonction `count_word`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'moins': 0, 'pas': 1, 'zéro': 2, 'tata': 3, 'mais': 4, 'zara': 5, 'la': 7, 'plus': 9, 'tête': 10, 'n': 6, 'toto': 8, 'à': 11, 'égale': 12}, array([[ 0.,  0.,  2.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  0.,  1.,  1.,  2.,  1.,  1.,  0.,  0.,  1.,  1.,  1.]]))\n"
     ]
    }
   ],
   "source": [
    "test0 = ['zéro plus zéro égale la tête à toto', 'mais zara moins zara n\\' égale pas la tête à tata']\n",
    "test0 = clean_data(test0)\n",
    "res0 = count_word(test0)\n",
    "print(res0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la fonction `count_word` sur une partie des documents sans que les données soient nettoyées dans un premier temps (c'est-à-dire la ponctuation et les caractères spéciaux sont inlcus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disappearances': 214, 'mold': 0, 'bringing': 1, 'excites': 144, 'i': 217, 'entire': 2, 'blair': 218, 'themselves': 3, 'work': 219, 'assuming': 4, 'tons': 327, \"bastard's\": 367, 'seem': 5, 'part': 216, 'jumbled': 221, 'robots': 222, 'mind': 223, '.': 6, 'has': 225, 'point': 226, 'what': 36, 'nightmares': 8, 'its': 9, 'world': 228, 'makes': 229, 'within': 11, 'y2k': 12, 'chopped': 13, 'wrapped': 15, 'movie': 195, 'accident': 16, 'parts': 44, 'starring': 232, 'arrow': 17, 'movies': 18, 'came': 234, 'engaging': 19, 'memento': 235, 'street': 236, 'is': 432, 'weird': 354, 'redundant': 21, 'entertain': 22, \"doesn't\": 220, 'five': 437, 'that': 23, 'review': 238, 'sense': 25, 'explanation': 26, 'middle': 240, 'nowhere': 27, 'people': 28, 'halloween': 29, 'up': 30, 'then': 241, 'lost': 31, 'stir': 32, 'decent': 33, 'folks': 242, 'took': 34, 'someone': 243, 'william': 224, 'kind': 244, 'skip': 245, \"someone's\": 342, 'offering': 428, 'jamie': 35, 'same': 246, 'personally': 374, '4/10': 247, 'half-way': 248, 'there': 37, 'oh': 251, 'are': 409, 'director': 252, 'critique': 253, 'like': 335, 'stumbling': 38, 'now': 39, 'correctly': 40, '9/10': 254, 'donald': 41, \"don't\": 255, 'likes': 256, 'american': 103, 'flashy': 257, 'salvation': 258, 'highway': 259, 'be': 42, \"character's\": 43, 'chase': 260, 'apparitions': 261, 'strangeness': 45, 'terribly': 46, 'chasing': 262, 'some': 263, 'cool': 362, \"it's\": 47, 'unravel': 364, 'melissa': 264, 'gore': 265, 'sunken': 49, 'thrilling': 50, 'likely': 352, 'dead': 266, 'making': 267, 'starts': 268, 'have': 184, 'ship': 7, 'films': 269, 'unraveling': 54, 'mess': 53, 'even': 378, '2': 295, 'problems': 272, 'ago': 227, 'final': 404, \"where's\": 55, 'bottom': 336, 'virus': 400, 'teen': 274, 'feeling': 57, 'concept': 277, 'so': 58, 'bentley': 59, 'somewhere': 60, 'playing': 61, 'the': 281, 'touches': 282, '20': 283, 'mind-fuck': 62, 'music': 63, 'big': 64, 'origin': 65, 'really': 66, 'my': 285, 'flicks': 280, 'kids': 286, 'secret': 412, 'bug': 287, 'continues': 288, 'giving': 67, 'completely': 289, 'presents': 290, 'snag': 291, 'brain': 10, 'figured': 68, 'insight': 292, 'coming': 435, 'shot': 384, 'wes': 69, '?': 70, 'way': 78, 'drive': 71, '8/10': 72, 'watch': 73, 'it': 293, 'got': 230, 'further': 294, 'real': 74, 'or': 79, 'across': 296, 'give': 297, 'attempt': 75, '10/10': 298, 'new': 351, 'girlfriend': 76, 'problem': 299, 'most': 77, 'here': 80, 'power': 300, 'every': 81, 'downshifts': 82, 'simply': 302, 'plot': 83, 'video': 107, 'package': 304, 'stick': 429, 'much': 231, 'design': 306, 'explained': 307, 'password': 308, 'holds': 14, 'seems': 385, 'guys': 309, 'couples': 310, 'happen': 311, 'suits': 312, 'plain': 86, 'h20': 315, 'joblo': 314, \"might've\": 316, 'years': 87, 'neat': 318, 'scenes': 386, 'all': 319, ':': 89, 'sorta': 320, \"let's\": 321, 'about': 90, 'substance': 303, 'know': 322, 'no': 91, \"you're\": 92, 'again': 323, 'course': 93, 'hot': 94, 'drunkenly': 418, 'sagemiller': 324, 'me': 388, 'main': 196, 'ways': 325, 'characters': 326, 'only': 96, 'picking': 97, 'deserted': 98, 'mean': 328, 'bad': 329, 'idea': 330, 'party': 233, 'we': 99, 'him': 331, 'and': 100, 'elm': 332, 'such': 333, 'head': 334, 'beauty': 101, 'from': 102, 'does': 337, 'rarely': 104, 'otherwise': 105, 'audience': 338, 'these': 106, 'generation': 340, 'hide': 341, 'guess': 301, 'look': 108, 'entertaining': 343, 'harder': 390, 'tugboat': 109, 'craziness': 344, 'lazy': 20, 'sutherland': 110, 'make': 348, 'two': 48, 'stan': 111, '\"': 345, 'dig': 346, 'with': 305, 'hey': 112, 'occasional': 347, 'dreams': 113, 'very': 349, 'away': 350, 'of': 114, 'although': 115, \"what's\": 116, 'would': 84, 'happy': 117, 'write': 118, 'by': 237, 'baldwin': 353, 'more': 119, 'applaud': 120, 'biggest': 51, 'horror': 121, 'empty': 355, 'hit': 122, 'well': 356, 'tech': 123, 'quick': 124, 'until': 85, 'he': 359, 'own': 125, 'back': 127, \"film's\": 129, 'who': 52, 'was': 363, 'sad': 270, 'time': 201, 'kick': 365, 'runtime': 131, 'generally': 366, 'minutes': 368, 'film': 369, '&': 132, 'flick': 133, 'packaged': 393, 'fed': 370, 'too': 134, 'feels': 371, 'one': 372, 'sure': 56, '!': 373, 'another': 135, 'brother': 136, 'flash': 137, 'should': 138, 'do': 249, 'an': 375, \"here's\": 139, 'out': 376, 'character': 377, 'if': 140, 'good': 141, 'nightmare': 380, 'while': 24, 'exact': 382, 'since': 142, 'you': 383, 'start': 239, 'going': 143, 'church': 271, 'understanding': 145, 'to': 146, 'sequences': 317, 'russian': 147, 'sitting': 148, 'into': 126, 'decided': 149, 'because': 387, 'taken': 150, 'but': 151, 'normal': 152, 'for': 153, 'little': 389, 'obviously': 154, 'see': 203, 'crew': 339, \"he's\": 250, 'drink': 155, 'clue': 313, 'both': 157, 'showing': 411, \"winston's\": 158, 'why': 426, 'been': 159, 'life': 392, 'slasher': 88, 'actors': 160, 'lee': 161, '7/10': 162, 'despite': 163, 'confusing': 275, 'others': 394, '(': 395, 'witch': 164, 'member': 165, 'regarding': 166, 'thing': 396, 'pretty': 397, 'strange': 167, 'want': 436, 'studio': 168, 'go': 169, 'this': 170, 'a': 171, \"didn't\": 276, '-': 172, 'pink': 439, 'on': 173, 'trying': 401, 'different': 174, 'just': 402, 'her': 403, 'did': 405, 'actually': 278, 'off': 175, 'shows': 176, 'schnazzy': 406, 'looooot': 177, 'throughout': 178, 'dies': 407, 'his': 279, 'given': 179, 'edge': 181, 'find': 182, 'they': 408, 'around': 410, 'whatever': 183, 'us': 379, 'visions': 357, 'after': 185, 'crow': 186, 'apparently': 187, 'cgi': 188, 'mir': 189, 'deal': 190, 'shelves': 391, 'always': 191, 'when': 413, 'comes': 414, 'which': 192, 'in': 358, ')': 180, 'as': 415, 'body': 416, 'production': 417, 'overall': 193, 'over': 194, 'neighborhood': 198, 'acting': 419, 'still': 197, 'below': 360, 'line': 420, 'star': 199, 'executed': 421, 'also': 422, 'echoes': 156, 'running': 423, 'your': 424, 'down': 425, 'before': 398, 'action': 200, 'seemed': 427, 'things': 128, 'need': 399, 'damn': 430, 'turn': 361, 'enter': 202, '3': 431, 'kudos': 433, 'wasted': 130, 'how': 434, 'fantasy': 273, 'turning': 204, 'ending': 205, 'break': 206, 'okay': 207, 'bit': 208, 'genre': 438, 'ever': 440, ',': 209, 'not': 441, 'get': 95, 'meantime': 210, 'types': 284, 'average': 211, 'curtis': 212, 'few': 381, 'robot': 215, 'story': 213}\n",
      "====================\n",
      "[[  1.   0.   1.   1.   1.   1.  34.   0.   1.   4.   0.   0.   0.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.  13.   1.   2.   1.   0.\n",
      "    1.   0.   2.   2.   1.   1.   1.   0.   2.  10.   0.   2.   1.   0.\n",
      "    1.   1.   0.   2.   1.   4.   2.   0.   1.   2.   3.   1.   1.   1.\n",
      "    1.   1.   3.   1.   1.   1.   2.   1.   1.   0.   2.   1.   1.   1.\n",
      "    6.   1.   1.   1.   0.   1.   1.   4.   2.   2.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   3.   2.   1.   0.   0.   1.   3.   1.   0.\n",
      "    0.   4.  20.   1.   4.   1.   1.   0.   1.   1.   2.   0.   0.   0.\n",
      "    0.   1.  16.   1.   2.   0.   1.   2.   1.   1.   0.   0.   0.   1.\n",
      "    5.   1.   2.   1.   0.   1.   1.   1.   1.   0.   0.   0.   1.   0.\n",
      "    0.   2.   2.   2.   1.   1.  16.   0.   1.   1.   1.  10.   1.   4.\n",
      "    1.   1.   1.   1.   0.   2.   1.   0.   2.   1.   1.   1.   0.   1.\n",
      "    1.   2.  10.  14.   7.   4.   1.   1.   1.   1.   2.   1.   9.   1.\n",
      "    1.   1.   2.   2.   2.   2.   0.   0.   1.   1.   4.   1.   4.   6.\n",
      "    1.   2.   1.   0.   0.   0.   1.   2.   1.   1.   1.   1.   1.  44.\n",
      "    1.   0.   0.   0.   1.   0.   2.   7.   1.   0.   2.   1.   0.   1.\n",
      "    0.   3.   1.   1.   2.   1.   1.   0.   0.   1.   1.   2.   1.   2.\n",
      "    1.   1.   0.   3.   1.   1.   1.   1.   2.   1.   1.   2.   0.   1.\n",
      "    1.   1.   2.   2.   0.   0.   1.   2.   1.   1.   1.   0.   1.   0.\n",
      "    2.   1.   1.   2.   1.   1.   1.   1.   4.   1.   2.   1.   2.   1.\n",
      "    1.  38.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.  21.\n",
      "    1.   1.   0.   1.   2.   2.   0.   2.   2.   0.   1.   5.   0.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   0.   1.   0.   1.   6.   1.   0.\n",
      "    1.   2.   2.   1.   1.   1.   1.   2.   2.   1.   1.   1.   1.   3.\n",
      "    1.   1.   2.   0.   1.   2.   0.   2.   1.  10.   1.   0.   5.   2.\n",
      "    2.   1.   0.   0.   1.   0.   1.   1.   8.   1.   0.   0.   2.   0.\n",
      "    1.   0.   1.   0.   2.   5.   1.   1.   3.   3.   1.   3.   3.   1.\n",
      "    3.   1.   1.   0.   1.   3.   0.   1.   2.   2.   1.   2.   1.   1.\n",
      "    1.   1.   2.   9.   0.   5.   2.   1.   0.   1.   4.   4.   1.   2.\n",
      "    0.   1.   5.  13.   0.   1.   2.   1.   0.   1.   0.   1.   0.   0.\n",
      "    1.   1.   1.   1.   2.   1.   0.   1.   1.   1.   0.   1.  12.   1.\n",
      "    1.   2.   1.   1.   1.   0.   1.   3.]\n",
      " [  0.   1.   0.   0.   0.   0.  14.   3.   0.   0.   1.   1.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.   0.   0.   1.\n",
      "    1.   1.   0.   0.   0.   0.   1.   1.   1.   1.   1.   0.   0.   1.\n",
      "    0.   0.   1.   1.   0.   3.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   1.   1.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   3.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   1.   2.   0.   1.   0.   1.\n",
      "    1.   3.   8.   0.   1.   0.   0.   1.   1.   0.   0.   1.   2.   1.\n",
      "    1.   0.   8.   0.   0.   1.   0.   1.   0.   0.   1.   1.   1.   0.\n",
      "    1.   1.   0.   0.   1.   0.   0.   0.   0.   1.   1.   1.   0.   1.\n",
      "    1.   1.   0.   2.   0.   0.   2.   1.   0.   0.   0.   0.   0.   2.\n",
      "    0.   0.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   2.  13.   0.   3.   0.   0.   0.   0.   1.   0.   2.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   1.   0.   0.   0.   0.   1.   5.\n",
      "    0.   1.   0.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.  18.\n",
      "    0.   1.   2.   1.   0.   1.   0.   0.   0.   1.   0.   0.   2.   0.\n",
      "    1.   1.   0.   0.   0.   0.   1.   1.   1.   0.   0.   0.   0.   0.\n",
      "    1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   1.   1.   0.\n",
      "    0.   0.   0.   3.   1.   1.   0.   0.   1.   0.   0.   2.   0.   2.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  13.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   1.   0.   0.   0.   2.   0.   0.   1.   0.   1.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   1.   0.   1.\n",
      "    4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   3.\n",
      "    0.   0.   0.   2.   0.   0.   1.   0.   0.   2.   0.   1.   0.   1.\n",
      "    0.   0.   1.   3.   0.   1.   1.   0.   4.   0.   1.   1.   0.   1.\n",
      "    0.   2.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.   2.   0.\n",
      "    1.   0.   0.   1.   0.   1.   1.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   2.   1.   1.   0.   0.   1.   0.   2.   1.   0.   0.\n",
      "    1.   0.   2.   1.   2.   0.   0.   1.   1.   0.   1.   0.   1.   2.\n",
      "    0.   0.   0.   0.   1.   0.   2.   0.   0.   0.   1.   0.   3.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "#on teste sur une partie des documents\n",
    "test = texts[0:2]\n",
    "voc, count = count_word(test)\n",
    "print(voc)\n",
    "print('====================')\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse à l'ensemble des documents maintenant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 1.238551378250122\n",
      "(2000, 39443)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "texts1 = clean_data(texts) \n",
    "vocabulary1, X1 = count_word(texts1)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(X1.shape)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** : Les documents ont été classés selon certaines notations: étoiles, lettres ou notes sur 5 ou sur 10. Pour chaque système d'évaluation, un seuil critique a été défini permettant de classer un document selon qu'il est au-dessus (positif) ou en dessous de ce seuil (négatif)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 **: On cherche à implémenter un classifieur de type $Naïve$ $Bayes$ avec l'algorithme suivant:\n",
    "![title](/notebooks/algoBayes.jpg \"Algo Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NB0(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_ = X\n",
    "        self.y_ = y   \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Calcul des probabilités conditionnelles\n",
    "        T = np.zeros((n_features, 2))\n",
    "        T[:, 0] = np.sum(X[y == 0], axis=0) + 1 \n",
    "        T[:, 1] = np.sum(X[y == 1], axis=0) + 1\n",
    "        T /= np.sum(T, axis=0)\n",
    "        self.prior_ = [(len(np.where(y==c)[0])*1.0)/ n_samples for c in [0, 1]]\n",
    "        self.condprob_ = T\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        score = np.zeros((n_samples, 2))\n",
    "        score[:, 0] = np.log(self.prior_[0])\n",
    "        score[:, 1] = np.log(self.prior_[1])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Pour chaque doc, calcul de la proba d'appartenir à la classe c\n",
    "            score[i, :] += np.sum(np.log(self.condprob_[X[i, :] != 0]), axis=0)\n",
    "        return np.argmax(score, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On teste le classifieur ainsi implémenté sur un dataset de petite dimension (cf vidéo Youtube https://www.youtube.com/watch?v=km2LoOpdB3A). On choisit le label ${1}$ pour la classe **Chinese** et le label ${0}$ pour la classe **Japan**. On veut pourvoir prédire la classe du groupe de mots `['beijing tokyo tokyo japan','japan hokkaido tokyo macao', 'macao shanghai chinese tokyo']` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.  0.  0.  0.  0.  1.]\n",
      " [ 2.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  2.  0.  0.  0.]]\n",
      "{'shanghai': 5, 'chinese': 0, 'tokyo': 1, 'hokkaido': 2, 'japan': 3, 'beijing': 6, 'macao': 4}\n"
     ]
    }
   ],
   "source": [
    "data = ['chinese beijing chinese','chinese chinese shanghai','chinese macao','tokyo japan chinese','hokkaido japan japan']\n",
    "X_china = count_word(data)[1]\n",
    "voc_china = count_word(data)[0]\n",
    "y_china = np.array([1,1,1,0,0])\n",
    "print(X_china)\n",
    "print(voc_china)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NB0()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NB0()\n",
    "nb.fit(X_china, y_china)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "data_test = ['beijing tokyo tokyo japan','japan hokkaido tokyo macao', 'macao shanghai chinese tokyo']\n",
    "X_test_china = count_word(data_test)[1]\n",
    "voc_test_china= count_word(data_test)[0]\n",
    "predict_china = nb.predict(X_test_china)\n",
    "print(predict_china)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test précédent est concluant sur un petit dataset. On peut passer au test proposé dans l'énoncé sur les données nettoyées ($ie$ sans la ponctuation) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39443)\n",
      "duration: 0.3694641590118408\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
      " 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
      " 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
      " 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "start = time.time()\n",
    "nb.fit(X1[::2], y[::2])\n",
    "predict_texts1 = nb.predict(X1[1::2])\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(predict_texts1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 0.14691495895385742\n",
      "le score obtenu pour les data traitées: 0.82\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "score_texts1 = nb.score(X1[1::2], y[1::2])\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print('le score obtenu pour les data traitées:', score_texts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut tenter de faire la même chose sur les data brutes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 1.230593204498291\n",
      "(2000, 50920)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vocabulary_raw, X_raw= count_word(texts)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(X_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 0.4640522003173828\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
      " 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1]\n",
      "nb de documents traités: 1000\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nb.fit(X_raw[::2], y[::2])\n",
    "predict_texts = nb.predict(X_raw[1::2])\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(predict_texts)\n",
    "print('nb de documents traités:', len(predict_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 0.16891264915466309\n",
      "le score obtenu pour les data brutes: 0.836\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "score_texts = nb.score(X_raw[1::2], y[1::2])\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print('le score obtenu pour les data brutes:', score_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate qu'on obtient un meilleur score avec les data brutes par rapport aux data nettoyées. La raison peut-être dû au fait qu'en enlevant toute la ponctuation, on n'a pas tenu compte des notes données aux films de type '9/10' ou '2/10' qui peuvent clairement être un indicateur sur la classe.    \n",
    "Pour revenir à l'algorithme $Naïve$ $Bayes$ qui a été implémenté, j'ai fait les choix suivants:  \n",
    "* La variable X à traiter est la matrice `count` obtenue par la fonction `count_word` et non la liste de documents d'apprentissage. Celle-ci est pré-traitée dans un premier temps pour obtenir la matrice, le vocabulaire\n",
    "* La fonction `fit` permet de calculer la fréquence des classes `prior` et les probabilités conditionnelles `cond_prob` par classe de documents. Dans la relation $condprob[t][c] \\leftarrow \\frac{T_{ct}+1}{\\sum_{t'} T_{ct'}+1}$, le terme $1$ permet d'attribuer une probabilité non nulle à des mots qui ne seraient pas dans l'ensemble d'apprentissage. Dans le classifieur $Naïve$ $Bayes$ implémenté dans `sklearn`, ce terme devient le paramètre $\\alpha$ qui peut-être modulé  \n",
    "* La fonction `predict` s'applique sur des données test et plus précisément la matrice `count` obtenus à partir des données test. Pour chaque document, chaque mot et pour chaque classe, on regarde la probabilité conditionnelle de ce mot à appartenir à une classe spécifique, on somme de façon logarithmique pour obtenir un score pour chaque document du test. On récupère ensuite l'$argmax$ de ce score qui va nous donner la classe la plus probable du document    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 **: On va évaluer les performances avec une cross-validation 5-fold. Pour cela, on peut utiliser la fonction `cross_val_score` de $sklearn$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 3.7883408069610596\n",
      "[ 0.855  0.825  0.78   0.81   0.78 ]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.81\n",
      "la déviation standard est: 0.0284604989415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "start = time.time()\n",
    "scores = cross_val_score(nb,X_raw[1::2], y[1::2], cv = 5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores.mean())\n",
    "print('la déviation standard est:', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 **: On modifie la fonction `count_word` qui va tenir compte des *stop-words* qui se trouve dans un fichier particulier `english.stop`  \n",
    "Dans un premier temps, on récupère un set de mots uniques de `english.stop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero', '']\n",
      "========================================\n",
      "taille de la liste stop_words: 572\n"
     ]
    }
   ],
   "source": [
    "path_stop = 'D:/WORK/Big_Data/MDI343/data_TextMining/english.stop'\n",
    "stop_words = open(path_stop).read().split('\\n')\n",
    "print(stop_words)\n",
    "print('========================================')\n",
    "print('taille de la liste stop_words:', len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_word_stop(doc, stop):\n",
    "    vocabulary = {}#dictionnaire contenant un mot du document et son indice colonne dans la matrice count\n",
    "    words = set(' '.join(doc).split())#liste sans répétition de mots du documents\n",
    "    words -= set(stop)\n",
    "    \n",
    "    n_samples = len(doc)\n",
    "    n_features = len(words)\n",
    "    \n",
    "    for j,w in enumerate(words):\n",
    "        vocabulary[w] = j\n",
    "\n",
    "    counts = np.zeros((n_samples,n_features))\n",
    "    for index_text, text in enumerate(doc):\n",
    "        for w in text.split():\n",
    "            if w in vocabulary:\n",
    "                counts[index_text, vocabulary[w]] += 1\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la fonction `count_word_stop` sur une partie de l'échantillon de documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 0.0\n",
      "{'disappearances': 134, 'characters': 218, 'mold': 0, 'bringing': 1, 'picking': 63, 'deserted': 64, 'running': 276, 'entire': 2, 'blair': 137, 'idea': 221, 'work': 138, 'assuming': 3, 'tons': 219, \"bastard's\": 245, 'elm': 222, 'part': 136, 'shelves': 258, 'pink': 284, 'jumbled': 139, 'beauty': 65, 'robots': 140, 'american': 66, '.': 4, 'thing': 262, 'point': 143, 'rarely': 67, 'ago': 144, 'audience': 225, 'video': 68, 'generation': 227, 'hide': 228, 'entertaining': 230, 'back': 81, 'nightmares': 6, 'brain': 7, 'h20': 211, 'tugboat': 69, 'world': 145, 'lazy': 16, 'sutherland': 70, 'chopped': 9, 'wrapped': 11, 'make': 235, 'accident': 12, '\"': 232, 'baldwin': 236, 'bad': 220, 'starring': 147, 'arrow': 13, 'movies': 14, 'hey': 72, 'engaging': 15, 'memento': 149, 'occasional': 234, 'dreams': 73, 'street': 150, 'weird': 237, 'password': 204, 'redundant': 17, 'entertain': 18, 'happy': 74, 'turn': 240, 'write': 75, 'review': 151, 'sense': 19, 'explanation': 20, 'figured': 47, 'middle': 153, 'insight': 192, 'echoes': 98, 'people': 21, 'halloween': 22, 'applaud': 76, 'starts': 173, 'horror': 77, 'empty': 238, 'hit': 78, 'tech': 79, 'quick': 80, 'lost': 23, 'stir': 24, 'pretty': 263, 'decent': 25, 'folks': 154, 'unravel': 242, 'mind': 141, 'kind': 155, \"film's\": 83, 'wasted': 84, \"someone's\": 229, 'sad': 175, 'types': 185, 'time': 123, 'kick': 243, 'runtime': 85, 'generally': 244, 'minutes': 246, 'jamie': 26, '?': 49, 'personally': 251, '4/10': 157, 'film': 247, 'half-way': 158, '&': 86, 'flick': 87, 'stick': 278, 'fed': 248, 'head': 223, 'feels': 249, 'director': 159, 'critique': 160, '!': 250, 'brother': 88, 'stumbling': 27, 'flash': 89, 'correctly': 28, '9/10': 161, 'donald': 29, 'likes': 162, 'william': 142, 'character': 252, 'joblo': 210, 'flashy': 163, 'salvation': 164, 'nightmare': 253, 'highway': 165, 'kudos': 281, 'shot': 255, 'start': 152, 'excites': 91, \"character's\": 30, 'understanding': 92, 'chase': 166, 'apparitions': 167, 'sequences': 213, 'russian': 93, 'sitting': 94, 'strangeness': 32, 'neat': 214, 'decided': 95, 'chasing': 168, 'normal': 96, 'cool': 241, 'confusing': 180, 'melissa': 169, 'gore': 170, 'harder': 257, 'sunken': 34, 'thrilling': 35, 'dead': 171, 'making': 172, 'drink': 97, 'biggest': 36, 'clue': 209, 'ship': 5, 'films': 174, 'sorta': 215, 'mess': 37, \"winston's\": 99, 'church': 176, 'unraveling': 38, 'happen': 207, 'actors': 100, 'lee': 101, '7/10': 102, 'packaged': 260, 'teen': 179, 'feeling': 39, 'party': 148, 'concept': 181, '(': 261, 'witch': 103, 'bentley': 40, 'good': 90, 'playing': 41, 'strange': 105, 'studio': 106, 'touches': 183, 'virus': 264, '-': 107, 'mind-fuck': 42, 'life': 259, 'music': 43, 'big': 44, 'origin': 45, 'flicks': 182, 'kids': 186, 'final': 265, 'bug': 187, 'continues': 188, 'exact': 254, 'giving': 46, 'completely': 189, 'presents': 190, 'snag': 191, 'shows': 108, 'schnazzy': 266, 'looooot': 109, ',': 129, 'dies': 267, 'dig': 233, 'member': 104, 'fantasy': 178, 'wes': 48, 'edge': 111, 'find': 112, 'production': 271, 'drive': 50, '8/10': 51, 'watch': 52, 'visions': 239, 'problems': 177, 'parts': 31, 'real': 53, '2': 193, 'crow': 113, 'apparently': 114, 'cgi': 115, 'mir': 116, 'deal': 117, 'attempt': 54, '10/10': 195, 'secret': 269, 'girlfriend': 55, 'problem': 196, 'y2k': 8, ')': 110, 'power': 197, 'guess': 198, 'body': 270, 'main': 119, 'downshifts': 56, 'scenes': 256, 'simply': 199, 'plot': 57, 'crew': 226, 'neighborhood': 120, 'acting': 273, 'package': 201, 'line': 274, 'star': 121, 'executed': 275, 'makes': 146, 'design': 202, 'explained': 203, 'give': 194, 'holds': 10, 'guys': 205, 'couples': 206, 'action': 122, 'suits': 208, 'things': 82, 'plain': 58, 'offering': 277, 'damn': 279, 'skip': 156, 'enter': 124, '20': 184, '3': 280, \"might've\": 212, 'years': 59, 'slasher': 60, 'bottom': 224, 'genre': 283, 'turning': 125, ':': 61, 'ending': 126, 'break': 127, 'showing': 268, 'bit': 128, 'terribly': 33, 'substance': 200, 'coming': 282, 'stan': 71, 'meantime': 130, 'hot': 62, 'craziness': 231, 'average': 131, 'drunkenly': 272, 'ways': 217, 'sagemiller': 216, 'curtis': 132, 'movie': 118, 'robot': 135, 'story': 133}\n",
      "====================\n",
      "[[  1.   0.   1.   1.  34.   0.   1.   0.   0.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   2.   1.   1.   0.   2.   1.   1.   0.   0.\n",
      "    1.   0.   1.   0.   2.   1.   0.   1.   2.   1.   1.   1.   1.   1.\n",
      "    2.   1.   1.   0.   1.   1.   1.   6.   1.   1.   1.   0.   1.   1.\n",
      "    1.   1.   1.   1.   1.   3.   1.   0.   0.   1.   1.   1.   1.   0.\n",
      "    0.   0.   0.   1.   0.   1.   1.   1.   0.   0.   0.   1.   2.   1.\n",
      "    0.   1.   1.   1.   0.   0.   2.   1.   1.   0.   1.   1.   1.   1.\n",
      "    1.   0.   1.   0.   2.   1.   1.   1.   1.   7.   1.   1.   9.   1.\n",
      "    1.   2.   2.   0.   0.   1.   6.   1.   1.   0.   0.   0.   1.   1.\n",
      "    1.   1.   1.  44.   1.   0.   0.   0.   1.   0.   2.   1.   0.   1.\n",
      "    0.   1.   0.   1.   1.   2.   1.   0.   1.   2.   1.   1.   1.   0.\n",
      "    1.   1.   1.   1.   1.   1.   1.   2.   0.   0.   1.   2.   1.   1.\n",
      "    1.   1.   0.   2.   1.   1.   2.   1.   1.   1.   1.   4.   1.   1.\n",
      "    1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.   2.\n",
      "    2.   0.   2.   2.   0.   1.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   0.   1.   0.   1.   1.   2.   1.   1.   1.   2.   2.   1.   1.\n",
      "    1.   2.   0.   1.   2.   0.   2.   1.  10.   1.   0.   5.   0.   1.\n",
      "    0.   1.   0.   2.   1.   0.   1.   0.   2.   5.   1.   1.   3.   1.\n",
      "    1.   1.   1.   0.   2.   1.   1.   1.   1.   9.   0.   5.   0.   1.\n",
      "    0.   1.   1.   2.   0.   1.   0.   0.   1.   1.   1.   1.   1.   0.\n",
      "    1.   1.   2.   1.   0.]\n",
      " [  0.   1.   0.   0.  14.   3.   0.   1.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.   0.   1.   1.\n",
      "    0.   1.   0.   1.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.   0.   0.   1.\n",
      "    2.   1.   1.   0.   1.   0.   0.   0.   1.   1.   1.   1.   0.   0.\n",
      "    1.   0.   0.   0.   1.   1.   1.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.\n",
      "    0.   0.   0.   1.   1.   0.   5.   0.   0.   1.   1.   1.   0.   0.\n",
      "    0.   0.   0.  18.   0.   1.   2.   1.   0.   1.   0.   0.   1.   0.\n",
      "    2.   0.   1.   0.   0.   0.   0.   1.   0.   0.   0.   1.   1.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.   1.   0.\n",
      "    0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   2.   0.   0.   1.   0.   0.   2.   0.   1.   0.   3.   0.\n",
      "    1.   0.   1.   0.   0.   2.   0.   1.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   2.   1.   1.   1.   0.\n",
      "    1.   0.   0.   0.   1.   0.   1.   2.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "voc_stop, count_stop  = count_word_stop(test, stop_words)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(voc_stop)\n",
    "print('====================')\n",
    "print(count_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du dictionnaire sans les mots stop: 285\n",
      "taille du dictionnaire avec les mots stop: 442\n"
     ]
    }
   ],
   "source": [
    "print('taille du dictionnaire sans les mots stop:',len(voc_stop))\n",
    "print('taille du dictionnaire avec les mots stop:',len(voc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien vérifié que la taille du dictionnaire est plus petite sans les *stop_words* qu'avec, ce qui parait logique.  \n",
    "On regarde si les performances sont améliorées en enlevant les mots stop avec notre classifieur NB et une cross_validation de 5_fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 1.0947341918945312\n",
      "(2000, 50375)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vocabulary_stop, X_stop = count_word_stop(texts, stop_words)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(X_stop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 3.869823694229126\n",
      "[ 0.845  0.805  0.77   0.85   0.78 ]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.81\n",
      "la déviation standard est: 0.0327108544676\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "scores_stop = cross_val_score(nb,X_stop[1::2], y[1::2], cv = 5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_stop)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_stop.mean())\n",
    "print('la déviation standard est:', scores_stop.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour conclure sur la **question 5**, on constate que le score est légérement détérioré par le fait de ne pas tenir compte des *stop_words* bien que cela reste du même ordre de magnitude.  \n",
    "A priori on peut se dire que ces mots rajoutent du bruit, c'est-à-dire qu'ils ne contribuent pas de façon déterminante à la détermination d'une classe de documents.  \n",
    "Cependant, on peut remarquer que certains mots de la liste *stop_words* peuvent nuancer une opinion et donc influer sur l'appartenance à une classe de tel ou tel documents. Par exemple *although*, *however* et toutes les conjonctions pouvant nuancer une opinion, les verbes à la forme négative tels que *don't* ou *couldn't* et certains adverbes comme *awfully*.  \n",
    "Ce qui laisse à penser qu'il faudrait en fait créer une troisième classe d'opinion neutre et refaire l'étude avec des labels ${-1,0,1}$ pour *négatif*, *neutre* ou *positif*, ce qui permettrait éventuellement d'affiner le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin... Utilisation de $sklearn$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**:\n",
    "On compare notre classifieur $NB0$ avec celui de $sklearn$. Pour ce faire, on va utiliser la classe `CountVectorizer` et un `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 5.0367231369018555\n",
      "[ 0.81   0.825  0.81   0.825  0.79 ]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.812\n",
      "la déviation standard est: 0.0128840987267\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('vectorizer', CountVectorizer()), ('nb_sk', MultinomialNB())])\n",
    "start = time.time()\n",
    "scores_sk = cross_val_score(pipeline, texts, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_sk)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_sk.mean())\n",
    "print('la déviation standard est:', scores_sk.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On constate qu'on obtient à peu de chose près les mêmes résultats que notre propre classifieur, bien que la déviation standard soit plus petite dans ce cas.  \n",
    "On avait obtenu précédemment :  \n",
    "[ 0.855  0.825  0.78   0.81   0.78 ]  \n",
    "la moyenne des scores obtenus par cross_validation est: 0.81  \n",
    "la déviation standard est: 0.0284604989415  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On s'intéresse maintenant aux bigrammes (cf. http://scikit-learn.org/stable/modules/feature_extraction.html). On va utiliser le paramètre `ngram_range` de `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 23.918751001358032\n",
      "[ 0.8175  0.8375  0.8225  0.8425  0.8325]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.8305\n",
      "la déviation standard est: 0.0092736184955\n"
     ]
    }
   ],
   "source": [
    "pipeline_bi = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))), ('nb_sk', MultinomialNB())])\n",
    "start = time.time()\n",
    "scores_bi = cross_val_score(pipeline_bi, texts, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_bi)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_bi.mean())\n",
    "print('la déviation standard est:', scores_bi.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le score obtenu est bien meilleur en prenant en compte les bigrammes. En effet (et je reprends la définition de **Wikipédia** ), un bigramme est une séquence de éléments adjacents d'une chaine de token (lettres, syllabes ou mots). La fréquence de distribution d'un bigramme dans une chaine permet de déterminer la probabilité d'un token, connaissant le token précédent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde maintenant les sous chaines de caractères et on ré-itère l'opération:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 153.97074460983276\n",
      "[ 0.8225  0.825   0.8175  0.845   0.7975]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.8215\n",
      "la déviation standard est: 0.0152151240547\n"
     ]
    }
   ],
   "source": [
    "pipeline_char = Pipeline([('vectorizer', CountVectorizer(analyzer='char', ngram_range=(3, 6))), ('nb_sk', MultinomialNB())])\n",
    "start = time.time()\n",
    "scores_char = cross_val_score(pipeline_char, texts, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_char)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_char.mean())\n",
    "print('la déviation standard est:', scores_char.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est légèrement moins bon qu'avec les bigrammes, dans la mesure où en analysant par chaines caratères(découpées par longueur de 3 à 6), on n'a plus la même organisation du texte qui a son importance. Le score est complètement dégradé si on ne précise pas la longueur des chaines de caractères à prendre en compte dans l'analyse (de l'ordre de 60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 ** : On va tester d'autres algorithmes.  \n",
    "Dans un premier temps on s'intéresse à la régression logistique qui est un modèle de régression binômiale. On garde l'analyse en bigrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 31.086959838867188\n",
      "[ 0.8225  0.8525  0.8525  0.87    0.865 ]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.8525\n",
      "la déviation standard est: 0.0165075740192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline_logit = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))), ('logistic', LogisticRegression())])\n",
    "start = time.time()\n",
    "scores_logit = cross_val_score(pipeline_logit, texts, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_logit)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_logit.mean())\n",
    "print('la déviation standard est:', scores_logit.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la régression logistique, on a un excellent score comparé au Naïve Bayes dans la mesure où la régression logistique permet de tenir compte des dépendances entre les attributs, ce qui peut être le cas dans une analyse sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste maintenant les supports vector machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 34.111642599105835\n",
      "[ 0.8175  0.845   0.8475  0.87    0.87  ]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.85\n",
      "la déviation standard est: 0.0194293592277\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "pipeline_svm = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))), ('linearSVC', svm.LinearSVC())])\n",
    "start = time.time()\n",
    "scores_svm = cross_val_score(pipeline_svm, texts, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_svm)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_svm.mean())\n",
    "print('la déviation standard est:', scores_svm.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient un résultat similaire à celui de la régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** : \n",
    "On va utiliser la librairie NLTK pour procéder à une *racinisation*. D'après **Wikipédia**, c'est un procédé de transformation des flexions en leur racine: la racine d’un mot correspond à la partie du mot restante une fois que l’on a supprimé son préfixe et suffixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_data(doc): \n",
    "    all_stem =[]\n",
    "    for index_text, text in enumerate(doc):\n",
    "        text_stem = []\n",
    "        for w in text.split():\n",
    "            text_stem.append(stemmer.stem(w))\n",
    "        s = ' '.join(text_stem)\n",
    "        all_stem.append(s) \n",
    "    return all_stem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la fonction `stem_data` sur une partie des data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 0.0157926082611084\n",
      "['plot : two teen coupl go to a church parti , drink and then drive . they get into an accid . one of the guy die , but his girlfriend continu to see him in her life , and has nightmar . what the deal ? watch the movi and \" sorta \" find out . . . critiqu : a mind-fuck movi for the teen generat that touch on a veri cool idea , but present it in a veri bad packag . which is what make this review an even harder one to write , sinc i general applaud film which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad way of make all type of film , and these folk just didn\\'t snag this one correct . they seem to have taken this pretti neat concept , but execut it terribl . so what are the problem with the movi ? well , it main problem is that it simpli too jumbl . it start off \" normal \" but then downshift into this \" fantasi \" world in which you , as an audienc member , have no idea what go on . there are dream , there are charact come back from the dead , there are other who look like the dead , there are strang apparit , there are disappear , there are a looooot of chase scene , there are ton of weird thing that happen , and most of it is simpli not explain . now i person don\\'t mind tri to unravel a film everi now and then , but when all it doe is give me the same clue over and over again , i get kind of fed up after a while , which is this film biggest problem . it obvious got this big secret to hide , but it seem to want to hide it complet until it final five minut . and do they make thing entertain , thrill or even engag , in the meantim ? not realli . the sad part is that the arrow and i both dig on flick like this , so we actual figur most of it out by the half-way point , so all of the strang after that did start to make a littl bit of sens , but it still didn\\'t the make the film all that more entertain . i guess the bottom line with movi like this is that you should alway make sure that the audienc is \" into it \" even befor they are given the secret password to enter your world of understand . i mean , show melissa sagemil run away from vision for about 20 minut throughout the movi is just plain lazi ! ! okay , we get it . . . there are peopl chase her and we don\\'t know who they are . do we realli need to see it over and over again ? how about give us differ scene offer further insight into all of the strang go down in the movi ? appar , the studio took this film away from it director and chop it up themselv , and it show . there might\\'v been a pretti decent teen mind-fuck movi in here somewher , but i guess \" the suit \" decid that turn it into a music video with littl edg , would make more sens . the actor are pretti good for the most part , although wes bentley just seem to be play the exact same charact that he did in american beauti , onli in a new neighborhood . but my biggest kudo go out to sagemil , who hold her own throughout the entir film , and actual has you feel her charact unravel . overal , the film doesn\\'t stick becaus it doesn\\'t entertain , it confus , it rare excit and it feel pretti redund for most of it runtim , despit a pretti cool end and explan to all of the crazi that came befor it . oh , and by the way , this is not a horror or teen slasher flick . . . it just packag to look that way becaus someon is appar assum that the genr is still hot with the kid . it also wrap product two year ago and has been sit on the shelv ever sinc . whatev . . . skip it ! where joblo come from ? a nightmar of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvat ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the other ( 9/10 ) - stir of echo ( 8/10 )', 'the happi bastard quick movi review damn that y2k bug . it got a head start in this movi star jami lee curti and anoth baldwin brother ( william this time ) in a stori regard a crew of a tugboat that come across a desert russian tech ship that has a strang to it when they kick the power back on . littl do they know the power within . . . go for the gore and bring on a few action sequenc here and there , virus still feel veri empti , like a movi go for all flash and no substanc . we don\\'t know whi the crew was realli out in the middl of nowher , we don\\'t know the origin of what took over the ship ( just that a big pink flashi thing hit the mir ) , and , of cours , we don\\'t know whi donald sutherland is stumbl around drunken throughout . here , it just \" hey , let chase these peopl around with some robot \" . the act is below averag , even from the like of curti . you\\'r more like to get a kick out of her work in halloween h20 . sutherland is wast and baldwin , well , he act like a baldwin , of cours . the real star here are stan winston robot design , some schnazzi cgi , and the occasion good gore shot , like pick into someon brain . so , if robot and bodi part realli turn you on , here your movi . otherwis , it pretti much a sunken ship of a movi .']\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "stem_texts_s = stem_data(texts[:2])\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(stem_texts_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après ce test, on applique la fonction à l'ensemble des documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 15.373677730560303\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "stem_texts = stem_data(texts)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(len(stem_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 1.1214759349822998\n",
      "taille du dictionnaire de stem: 33294\n",
      "taille du dictionnaire data brutes: 50920\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "voc_stem, count_stem = count_word(stem_texts)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print('taille du dictionnaire de stem:',len(voc_stem))\n",
    "print('taille du dictionnaire data brutes:', len(vocabulary_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 4.520931005477905\n",
      "[ 0.795   0.815   0.8     0.8325  0.8025]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.809\n",
      "la déviation standard est: 0.0134721935853\n"
     ]
    }
   ],
   "source": [
    "pipeline_stem = Pipeline([('vectorizer', CountVectorizer()), ('nb_sk', MultinomialNB())])\n",
    "start = time.time()\n",
    "scores_stem = cross_val_score(pipeline_stem, stem_texts, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_stem)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_stem.mean())\n",
    "print('la déviation standard est:', scores_stem.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par rapport au classifieur NB comprenant les bigrammes et dont le score était de 0.8305, cette implémentation est un peu moins bonne, bien que la déviation standard soit meilleure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: On va filtrer les mots par catégorie grammaticale (POS : Part Of Speech) et ne garder que les noms NM, les verbes V, les adverbes ADV et les adjectifs ADJ pour la classiﬁcation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "def token_tag(doc):\n",
    "    list_to_keep =['JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', ' RBR',\n",
    "                'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    all_tags = []\n",
    "    for index_text, text in enumerate(doc):\n",
    "        text_tag = []\n",
    "        tag_tokeep = []\n",
    "        for w in text.split():\n",
    "            text_tag.append(w)\n",
    "        tags = pos_tag(text_tag)\n",
    "        tag_tokeep = [t[0] for t in tags if t[1] in list_to_keep]\n",
    "        s = ' '.join(tag_tokeep)\n",
    "        all_tags.append(s) \n",
    "    return all_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la fonction `token_tag` sur une partie des documents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot teen couples go church party drink then drive get accident guys dies girlfriend continues see life has nightmares what\\'s deal watch movie \" sorta \" find critique mind-fuck movie teen generation touches very cool idea presents very bad package is makes review even write i generally applaud films attempt break mold mess head such lost highway memento are good bad ways making types films folks just didn\\'t snag correctly seem have taken pretty neat concept executed terribly are problems movie well main problem is it\\'s simply too jumbled starts \" normal \" then downshifts \" fantasy \" world audience member have idea what\\'s going are dreams are characters coming back dead are others look dead are strange apparitions are disappearances are looooot chase scenes are tons weird things happen most is simply not explained now i personally don\\'t mind trying unravel film now then does is give same clue over again i get kind fed while is film\\'s biggest problem it\\'s obviously got big secret hide seems want hide completely final minutes do make things entertaining thrilling even engaging meantime not really sad part is arrow i dig flicks actually figured most half-way point strangeness did start make little bit sense still didn\\'t make film more entertaining i guess bottom line movies is should always make sure audience is \" \" even are given secret password enter world understanding i mean showing melissa sagemiller running away visions minutes movie is just plain lazy okay get are people chasing don\\'t know are do really need see over again giving different scenes offering further insight strangeness going movie apparently studio took film away director chopped shows might\\'ve been pretty decent teen mind-fuck movie here somewhere i guess \" suits \" decided turning music video little edge would make more sense actors are pretty good most part wes bentley just seemed be playing exact same character did american beauty only new neighborhood biggest kudos go sagemiller holds own entire film actually has feeling character\\'s unraveling overall film doesn\\'t stick doesn\\'t entertain it\\'s confusing rarely excites feels pretty redundant most runtime pretty cool ending explanation craziness came way is not horror teen slasher flick it\\'s just packaged look way someone is apparently assuming genre is still hot kids also wrapped production years ago has been sitting shelves ever whatever skip where\\'s joblo coming nightmare elm street blair witch crow crow salvation lost highway memento others stir echoes', 'happy bastard\\'s quick movie review damn y2k bug it\\'s got head start movie starring jamie lee curtis baldwin brother time story regarding crew tugboat comes deserted russian tech ship has strangeness kick power back little do know power going gore bringing few action sequences here there virus still feels very empty movie going flash substance don\\'t know crew was really middle nowhere don\\'t know origin took ship just big pink flashy thing hit mir course don\\'t know donald sutherland is stumbling drunkenly here it\\'s just \" hey let\\'s chase people robots \" acting is below average even likes curtis likely get kick work halloween h20 sutherland is wasted baldwin well he\\'s acting baldwin course real star here are stan winston\\'s robot design schnazzy cgi occasional good gore shot picking someone\\'s brain so robots body parts really turn here\\'s movie otherwise it\\'s pretty much sunken ship movie']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(token_tag(test))\n",
    "print(len(token_tag(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 65.42023015022278\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "token_text = token_tag(texts)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(len(token_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique ensuite le pipeline (CountVectorizer, MultinomialNB) au data auxquelles on a appliqué le filtre de catégorisation grammaticale):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 3.3860716819763184\n",
      "[ 0.8025  0.8175  0.81    0.835   0.7925]\n",
      "la moyenne des scores obtenus par cross_validation est: 0.8115\n",
      "la déviation standard est: 0.0143701078632\n"
     ]
    }
   ],
   "source": [
    "pipeline_pos = Pipeline([('vectorizer', CountVectorizer()), ('nb_sk', MultinomialNB())])\n",
    "start = time.time()\n",
    "scores_pos = cross_val_score(pipeline_pos, token_text, y, cv=5)\n",
    "end = time.time()\n",
    "print('duration:', end-start)\n",
    "print(scores_pos)\n",
    "print('la moyenne des scores obtenus par cross_validation est:', scores_pos.mean())\n",
    "print('la déviation standard est:', scores_pos.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:  \n",
    "  \n",
    "Pour conclure ce TP, on voit que les différents algorithmes renvoient sensiblement les mêmes scores. Certains traitements de données peuvent détériorer le score (comme un pré-traitement pour enlever la ponctuation par exemple). La régression logistique a obtenu le meilleur score suivi de la SVM mais le Naive Bayes reste un bon classifieur.\n",
    "  \n",
    "|Algorithme                             | Temps d'execution (s)| Score | std   |\n",
    "| ------------------------------------- |:--------------------:| -----:|------:|\n",
    "| NB home_made                          | 2.51                 | 0.836 | NaN   |\n",
    "| NB home_made, cross_val               | 3.78                 |   0.81|0.02846|\n",
    "| NB home_made, cross_val, stop_word    | 4.97                 |   0.81|0.03271|\n",
    "| NB skl, cross_val                     | 5.03                 |  0.812|0.01288|\n",
    "| NB skl, cross_val, bigrammes          | 23.92                | 0.8305|0.00927|\n",
    "| NB skl, cross_val, char               | 153.9707             | 0.8215|0.01522|\n",
    "| Logit, cross_val, bigrammes           | 31.09                | 0.8525|0.01651|\n",
    "| SVM, cross_val, bigrammes             | 34.1                 |  0.85 |0.01942|\n",
    "| NB skl, cross_val, Snowball           | 21.01                | 0.809 |0.01347|\n",
    "| NB skl, cross_val, POS                | 68.81                | 0.8115|0.01437|"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
